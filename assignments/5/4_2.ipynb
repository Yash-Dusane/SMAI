{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQG1JrooV2a_"
   },
   "source": [
    "----------------------\n",
    "## 4.2 Optical Character Recognition\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEqLDc_wS-QS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from nltk.corpus import words\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFIQ8bhBTuG1"
   },
   "outputs": [],
   "source": [
    "\n",
    "IMG_W, IMG_H = 256, 64\n",
    "FONT_SIZE = 32\n",
    "BLANK = \"_\"\n",
    "MAX_LEN = 30\n",
    "DATASET_SIZE = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_img_with_text(text):\n",
    "    img = Image.new(\"L\", (IMG_W, IMG_H), color=255)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\", FONT_SIZE)\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_w, text_h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    x, y = (IMG_W - text_w) // 2, (IMG_H - text_h) // 2\n",
    "    draw.text((x, y), text, font=font, fill=0)\n",
    "    return np.array(img)\n",
    "\n",
    "def create_uniform_dataset():\n",
    "    alphabet_groups = {chr(i): [] for i in range(97, 123)}  # 'a' to 'z'\n",
    "    for word in words.words():\n",
    "        if len(word) <= MAX_LEN and word[0].isalpha():\n",
    "            alphabet_groups[word[0].lower()].append(word)\n",
    "\n",
    "    selected_words = []\n",
    "    words_per_group = DATASET_SIZE // len(alphabet_groups)\n",
    "    for group, word_list in alphabet_groups.items():\n",
    "        selected_words.extend(random.sample(word_list, min(len(word_list), words_per_group)))\n",
    "\n",
    "    imgs, lbls = [], []\n",
    "    for word in selected_words:\n",
    "        word_padded = word.ljust(MAX_LEN, BLANK)\n",
    "        imgs.append(gen_img_with_text(word))\n",
    "        lbls.append(word_padded)\n",
    "\n",
    "    imgs = np.array(imgs).reshape(-1, 1, IMG_H, IMG_W) / 255.0\n",
    "    return imgs, lbls\n",
    "\n",
    "imgs, lbls = create_uniform_dataset()\n",
    "train_imgs, val_imgs, train_lbls, val_lbls = train_test_split(imgs, lbls, test_size=0.2, random_state=42)\n",
    "\n",
    "all_chars = sorted(set(\"\".join(lbls)))\n",
    "char_to_idx = {c: i for i, c in enumerate(all_chars)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "NUM_CLASSES = len(all_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A51GEZplTyMR"
   },
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, imgs, lbls):\n",
    "        self.imgs = torch.tensor(imgs, dtype=torch.float32)\n",
    "        self.lbls = torch.tensor([[char_to_idx[c] for c in label] for label in lbls], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.lbls[idx]\n",
    "\n",
    "train_dataset = OCRDataset(train_imgs, train_lbls)\n",
    "val_dataset = OCRDataset(val_imgs, val_lbls)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gIe7y08T31t"
   },
   "outputs": [],
   "source": [
    "class OptimizedCRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OptimizedCRNN, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(32), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(64), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.BatchNorm2d(128), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(128 * 8 * 32, 256)\n",
    "        self.rnn = nn.GRU(input_size=256, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.output_layer = nn.Linear(256, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        x = x.unsqueeze(1).repeat(1, MAX_LEN, 1)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = OptimizedCRNN(NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azTDKCYJDjiF",
    "outputId": "4adc80e6-e8e8-438b-a93c-b66d2acc51b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch               1/25    | Model Avg Correct   0.7207 | Random Baseline     0.0182\n",
      "Train Loss          1.0766 | Val Loss            0.9656\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: oooooooooiis                   | Actual: monumentalism\n",
      "Prediction: aaai                           | Actual: worth\n",
      "Prediction: oooooooooiis                   | Actual: ethnobiological\n",
      "Prediction: aaaaiii                        | Actual: xyphoid\n",
      "Prediction: aaaaiii                        | Actual: quiritary\n",
      "\n",
      "Epoch               2/25    | Model Avg Correct   0.7219 | Random Baseline     0.0185\n",
      "Train Loss          0.9611 | Val Loss            0.9621\n",
      "\n",
      "Epoch               3/25    | Model Avg Correct   0.7176 | Random Baseline     0.0188\n",
      "Train Loss          0.9410 | Val Loss            0.9759\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: aaooooiiiie                    | Actual: monumentalism\n",
      "Prediction: yaae                           | Actual: worth\n",
      "Prediction: peooooooiiiiie                 | Actual: ethnobiological\n",
      "Prediction: yaaae                          | Actual: xyphoid\n",
      "Prediction: yaaaiee                        | Actual: quiritary\n",
      "\n",
      "Epoch               4/25    | Model Avg Correct   0.7204 | Random Baseline     0.0182\n",
      "Train Loss          0.9367 | Val Loss            0.9519\n",
      "\n",
      "Epoch               5/25    | Model Avg Correct   0.7094 | Random Baseline     0.0186\n",
      "Train Loss          0.9351 | Val Loss            0.9878\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: nennnooooooiiies               | Actual: monumentalism\n",
      "Prediction: jaoee                          | Actual: worth\n",
      "Prediction: nennnooooooiiiiie              | Actual: ethnobiological\n",
      "Prediction: jeroeeee                       | Actual: xyphoid\n",
      "Prediction: oerooeie                       | Actual: quiritary\n",
      "\n",
      "Epoch               6/25    | Model Avg Correct   0.7228 | Random Baseline     0.0183\n",
      "Train Loss          0.9211 | Val Loss            0.9387\n",
      "\n",
      "Epoch               7/25    | Model Avg Correct   0.7280 | Random Baseline     0.0183\n",
      "Train Loss          0.9138 | Val Loss            0.9112\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: penrooooiiiie                  | Actual: monumentalism\n",
      "Prediction: kaae                           | Actual: worth\n",
      "Prediction: penroooooiiiiie                | Actual: ethnobiological\n",
      "Prediction: wereiie                        | Actual: xyphoid\n",
      "Prediction: wereiiee                       | Actual: quiritary\n",
      "\n",
      "Epoch               8/25    | Model Avg Correct   0.7259 | Random Baseline     0.0185\n",
      "Train Loss          0.9112 | Val Loss            0.9261\n",
      "\n",
      "Epoch               9/25    | Model Avg Correct   0.7282 | Random Baseline     0.0191\n",
      "Train Loss          0.9089 | Val Loss            0.9127\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: nenrooooiiiii                  | Actual: monumentalism\n",
      "Prediction: kaaie                          | Actual: worth\n",
      "Prediction: penooooooiiiis                 | Actual: ethnobiological\n",
      "Prediction: gereiie                        | Actual: xyphoid\n",
      "Prediction: gerreiie                       | Actual: quiritary\n",
      "\n",
      "Epoch               10/25    | Model Avg Correct   0.7272 | Random Baseline     0.0178\n",
      "Train Loss          0.9068 | Val Loss            0.9103\n",
      "\n",
      "Epoch               11/25    | Model Avg Correct   0.7297 | Random Baseline     0.0181\n",
      "Train Loss          0.9017 | Val Loss            0.9024\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: penooooooiiiii                 | Actual: monumentalism\n",
      "Prediction: waaiee                         | Actual: worth\n",
      "Prediction: penrooooooiiiiis               | Actual: ethnobiological\n",
      "Prediction: fereiie                        | Actual: xyphoid\n",
      "Prediction: ferriiie                       | Actual: quiritary\n",
      "\n",
      "Epoch               12/25    | Model Avg Correct   0.7303 | Random Baseline     0.0192\n",
      "Train Loss          0.8977 | Val Loss            0.8993\n",
      "\n",
      "Epoch               13/25    | Model Avg Correct   0.7295 | Random Baseline     0.0181\n",
      "Train Loss          0.8932 | Val Loss            0.9130\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: penoooooooiiie                 | Actual: monumentalism\n",
      "Prediction: jalie                          | Actual: worth\n",
      "Prediction: ienrooooooiiiiie               | Actual: ethnobiological\n",
      "Prediction: fatiiiie                       | Actual: xyphoid\n",
      "Prediction: fitiiiiie                      | Actual: quiritary\n",
      "\n",
      "Epoch               14/25    | Model Avg Correct   0.7286 | Random Baseline     0.0178\n",
      "Train Loss          0.8904 | Val Loss            0.9257\n",
      "\n",
      "Epoch               15/25    | Model Avg Correct   0.7169 | Random Baseline     0.0179\n",
      "Train Loss          0.8869 | Val Loss            0.9568\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: ienrottiiiiiii                 | Actual: monumentalism\n",
      "Prediction: fiiii                          | Actual: worth\n",
      "Prediction: nenoooooooooiiis               | Actual: ethnobiological\n",
      "Prediction: fitiiiie                       | Actual: xyphoid\n",
      "Prediction: fetiiiiie                      | Actual: quiritary\n",
      "\n",
      "Epoch               16/25    | Model Avg Correct   0.7352 | Random Baseline     0.0194\n",
      "Train Loss          0.8800 | Val Loss            0.8892\n",
      "\n",
      "Epoch               17/25    | Model Avg Correct   0.7241 | Random Baseline     0.0190\n",
      "Train Loss          0.8762 | Val Loss            0.9261\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: penoooooooiiis                 | Actual: monumentalism\n",
      "Prediction: fiiii                          | Actual: worth\n",
      "Prediction: nenoooooooooiiis               | Actual: ethnobiological\n",
      "Prediction: geneeiee                       | Actual: xyphoid\n",
      "Prediction: fitiiiiit                      | Actual: quiritary\n",
      "\n",
      "Epoch               18/25    | Model Avg Correct   0.7365 | Random Baseline     0.0184\n",
      "Train Loss          0.8739 | Val Loss            0.8778\n",
      "\n",
      "Epoch               19/25    | Model Avg Correct   0.7381 | Random Baseline     0.0187\n",
      "Train Loss          0.8715 | Val Loss            0.8752\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: nenooooiiiiis                  | Actual: monumentalism\n",
      "Prediction: yaaae                          | Actual: worth\n",
      "Prediction: ienooooooiiiiis                | Actual: ethnobiological\n",
      "Prediction: wanaaie                        | Actual: xyphoid\n",
      "Prediction: firiiiiee                      | Actual: quiritary\n",
      "\n",
      "Epoch               20/25    | Model Avg Correct   0.7335 | Random Baseline     0.0186\n",
      "Train Loss          0.8696 | Val Loss            0.9225\n",
      "\n",
      "Epoch               21/25    | Model Avg Correct   0.7383 | Random Baseline     0.0194\n",
      "Train Loss          0.8675 | Val Loss            0.8733\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: ienooooiiiiiis                 | Actual: monumentalism\n",
      "Prediction: filie                          | Actual: worth\n",
      "Prediction: penoooooooiiiss                | Actual: ethnobiological\n",
      "Prediction: wanaaie                        | Actual: xyphoid\n",
      "Prediction: fitiiiit                       | Actual: quiritary\n",
      "\n",
      "Epoch               22/25    | Model Avg Correct   0.7387 | Random Baseline     0.0192\n",
      "Train Loss          0.8657 | Val Loss            0.8701\n",
      "\n",
      "Epoch               23/25    | Model Avg Correct   0.7384 | Random Baseline     0.0186\n",
      "Train Loss          0.8643 | Val Loss            0.8702\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: penooooooaiis                  | Actual: monumentalism\n",
      "Prediction: kalie                          | Actual: worth\n",
      "Prediction: ieneoooiiiiiiis                | Actual: ethnobiological\n",
      "Prediction: wanaaie                        | Actual: xyphoid\n",
      "Prediction: fitiiiie                       | Actual: quiritary\n",
      "\n",
      "Epoch               24/25    | Model Avg Correct   0.7374 | Random Baseline     0.0182\n",
      "Train Loss          0.8637 | Val Loss            0.8723\n",
      "\n",
      "Epoch               25/25    | Model Avg Correct   0.7386 | Random Baseline     0.0196\n",
      "Train Loss          0.8629 | Val Loss            0.8672\n",
      "\n",
      "Sample Predictions:\n",
      "Prediction: penooooooaiis                  | Actual: monumentalism\n",
      "Prediction: kalie                          | Actual: worth\n",
      "Prediction: ienrriiiiiiiiis                | Actual: ethnobiological\n",
      "Prediction: wanaaee                        | Actual: xyphoid\n",
      "Prediction: firiiiiee                      | Actual: quiritary\n"
     ]
    }
   ],
   "source": [
    "def eval_rand_baseline(val_loader, char_to_idx):\n",
    "    correct, total = 0, 0\n",
    "    for imgs, lbls in val_loader:\n",
    "        rand_preds = torch.randint(len(char_to_idx), (imgs.size(0), MAX_LEN))\n",
    "        correct += (rand_preds == lbls).sum().item()\n",
    "        total += lbls.numel()\n",
    "    avg_correct = correct / total\n",
    "    # print(f\"\\n{'Random Baseline':<20} | Avg Correct Chars: {avg_correct:.4f}\")\n",
    "    return avg_correct\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, lbls in train_loader:\n",
    "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = 0\n",
    "            for t in range(MAX_LEN):\n",
    "                loss += criterion(outputs[:, t, :], lbls[:, t])\n",
    "            loss /= MAX_LEN\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct_chars, total_chars = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in val_loader:\n",
    "                imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                loss = 0\n",
    "                for t in range(MAX_LEN):\n",
    "                    loss += criterion(outputs[:, t, :], lbls[:, t])\n",
    "                loss /= MAX_LEN\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=2)\n",
    "                correct_chars += (preds == lbls).sum().item()\n",
    "                total_chars += lbls.numel()\n",
    "\n",
    "        avg_correct = correct_chars / total_chars\n",
    "        rand_baseline_avg = eval_rand_baseline(val_loader, char_to_idx)\n",
    "\n",
    "        print(f\"\\n{'Epoch':<20}{epoch+1}/{epochs:<5} | {'Model Avg Correct':<20}{avg_correct:.4f} | \"\n",
    "              f\"{'Random Baseline':<20}{rand_baseline_avg:.4f}\")\n",
    "        print(f\"{'Train Loss':<20}{train_loss / len(train_loader):.4f} | {'Val Loss':<20}{val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "              sample_imgs, sample_lbls = next(iter(val_loader))\n",
    "              sample_imgs, sample_lbls = sample_imgs.to(DEVICE), sample_lbls.to(DEVICE)\n",
    "              outputs = model(sample_imgs)\n",
    "              preds = torch.argmax(outputs, dim=2).cpu().numpy()\n",
    "              print(\"\\nSample Predictions:\")\n",
    "              for i in range(5):\n",
    "\n",
    "                  pred_text = \"\".join(idx_to_char[idx] for idx in preds[i] if idx != char_to_idx[BLANK]).rstrip(BLANK)\n",
    "                  true_text = \"\".join(idx_to_char[idx] for idx in sample_lbls[i].cpu().numpy()).rstrip(BLANK)\n",
    "                  print(f\"Prediction: {pred_text:<30} | Actual: {true_text}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4KMN-DxUQk5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
